{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-baacb7d78650>:9: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../RNN/tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../RNN/tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../RNN/tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../RNN/tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "''' basic package '''\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "''' tensorflow package '''\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.contrib import rnn\n",
    "mnist = input_data.read_data_sets(\"../RNN/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "Data shape:  (784,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f71ac2d6080>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#show training data\n",
    "print('Label: ', mnist.train.labels[1])\n",
    "print('Data shape: ', mnist.train.images[1].shape)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.imshow(mnist.train.images[1].reshape(28, 28), cmap='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = 28\n",
    "n_input = 28\n",
    "hidden_layer_size = 32\n",
    "number_of_layers = 1\n",
    "dropout = True\n",
    "dropout_rate=0.8\n",
    "learning_rate = 0.001\n",
    "n_classes = 10\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_cell(hidden_layer_size, X, number_of_layers, dropout=True, dropout_rate=0.8):\n",
    "    def get_RNN(hidden_layer_size, dropout, dropout_rate):\n",
    "        layer = tf.contrib.rnn.BasicRNNCell(hidden_layer_size)\n",
    "\n",
    "        if dropout:\n",
    "            layer = tf.contrib.rnn.DropoutWrapper(layer, output_keep_prob=dropout_rate)\n",
    "        return layer\n",
    "        \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([get_RNN(hidden_layer_size, dropout, dropout_rate) for _ in range(number_of_layers)])\n",
    "    init_state = cell.zero_state(tf.shape(X)[0], tf.float32)\n",
    "    \n",
    "    return cell, init_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow 靜態圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_graph = tf.Graph()\n",
    "sess = tf.Session(graph=main_graph)\n",
    "\n",
    "with main_graph.as_default():\n",
    "    \n",
    "    with tf.name_scope('inputs'):\n",
    "        X = tf.placeholder(tf.float32, shape=[None, time_step, n_input], name='X')\n",
    "        y = tf.placeholder(tf.float32, shape=[None, n_classes], name='y')\n",
    "        keep_prob = tf.placeholder(tf.float32, shape=[], name='keep_prob')\n",
    "        \n",
    "    with tf.variable_scope('RNN_layer'):\n",
    "        cell, init_state = RNN_cell(hidden_layer_size, X, number_of_layers, dropout, keep_prob)\n",
    "        outputs, states = tf.nn.dynamic_rnn(cell, X, initial_state=init_state)\n",
    "        \n",
    "    with tf.variable_scope('output_layer'):\n",
    "        RNN_last_outputs = outputs[:, -1, :]\n",
    "        prediction = tf.layers.dense(inputs=RNN_last_outputs, units=10)\n",
    "        \n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=prediction))\n",
    "        \n",
    "    with tf.name_scope('optimizer'):\n",
    "        opti = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "        \n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))\n",
    "        \n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  200\n",
      "Accuracy:  0.578125 , Loss:  1.31136\n",
      "----------------------------------\n",
      "Iter:  400\n",
      "Accuracy:  0.632812 , Loss:  1.0185\n",
      "----------------------------------\n",
      "Iter:  600\n",
      "Accuracy:  0.820312 , Loss:  0.623844\n",
      "----------------------------------\n",
      "Iter:  800\n",
      "Accuracy:  0.757812 , Loss:  0.864497\n",
      "----------------------------------\n",
      "Iter:  1000\n",
      "Accuracy:  0.6875 , Loss:  0.886214\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "sess.run(init)\n",
    "\n",
    "iter = 1\n",
    "while iter < 1001:\n",
    "    batch_X, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "    \n",
    "    batch_X = batch_X.reshape((batch_size, time_step, n_input))\n",
    "    \n",
    "    sess.run(opti, feed_dict={X: batch_X, y: batch_y, keep_prob: 0.8})\n",
    "    \n",
    "    if iter % 200 == 0:\n",
    "        acc, loss_ = sess.run([accuracy, loss], feed_dict={X: batch_X, y: batch_y, keep_prob: 0.8})\n",
    "#         acc=sess.run(accuracy,feed_dict={X:batch_X,y:batch_y, keep_prob: 0.8})\n",
    "        print('Iter: ', iter)\n",
    "        print('Accuracy: ', acc, ', Loss: ', loss_)\n",
    "        print('----------------------------------')\n",
    "    iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.773438\n"
     ]
    }
   ],
   "source": [
    "test_X = mnist.test.images[:128].reshape((-1, time_step, n_input))\n",
    "test_y = mnist.test.labels[:128]\n",
    "test_acc = sess.run(accuracy, feed_dict={X: test_X, y: test_y, keep_prob: 1.0})\n",
    "print('Test accuracy: ', test_acc)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習\n",
    "1. 把RNN cell換成LSTM觀察Accuracy是否會上升?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_cell(hidden_layer_size, X, number_of_layers, dropout=True, dropout_rate=0.8):\n",
    "    def get_LSTM(hidden_layer_size, dropout, dropout_rate):\n",
    "#         layer = tf.contrib.rnn.LSTMCell(hidden_layer_size)\n",
    "        layer = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size)\n",
    "\n",
    "        if dropout:\n",
    "            layer = tf.contrib.rnn.DropoutWrapper(layer, output_keep_prob=dropout_rate)\n",
    "        return layer\n",
    "        \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([get_LSTM(hidden_layer_size, dropout, dropout_rate) for _ in range(number_of_layers)])\n",
    "    init_state = cell.zero_state(tf.shape(X)[0], tf.float32)\n",
    "    \n",
    "    return cell, init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_graph = tf.Graph()\n",
    "sess = tf.Session(graph=main_graph)\n",
    "\n",
    "with main_graph.as_default():\n",
    "    \n",
    "    with tf.name_scope('inputs'):\n",
    "        X = tf.placeholder(tf.float32, shape=[None, time_step, n_input], name='X')\n",
    "        y = tf.placeholder(tf.float32, shape=[None, n_classes], name='y')\n",
    "        keep_prob = tf.placeholder(tf.float32, shape=[], name='keep_prob')\n",
    "        \n",
    "    with tf.variable_scope('LSTM_layer'):\n",
    "        cell, init_state = LSTM_cell(hidden_layer_size, X, number_of_layers, dropout, keep_prob)\n",
    "        outputs, states = tf.nn.dynamic_rnn(cell, X, initial_state=init_state)\n",
    "        \n",
    "    with tf.variable_scope('output_layer'):\n",
    "        LSTM_last_outputs = outputs[:, -1, :]\n",
    "        prediction = tf.layers.dense(inputs=LSTM_last_outputs, units=n_classes)\n",
    "        \n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=y))\n",
    "        \n",
    "    with tf.name_scope('optimizer'):\n",
    "        opti = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "        \n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  200\n",
      "Accuracy:  0.703125 , Loss:  0.892288\n",
      "----------------------------------\n",
      "Iter:  400\n",
      "Accuracy:  0.84375 , Loss:  0.556373\n",
      "----------------------------------\n",
      "Iter:  600\n",
      "Accuracy:  0.9375 , Loss:  0.273426\n",
      "----------------------------------\n",
      "Iter:  800\n",
      "Accuracy:  0.929688 , Loss:  0.284311\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "sess.run(init)\n",
    "\n",
    "iter = 1\n",
    "while iter < 1001:\n",
    "    \n",
    "    batch_X, batch_y = mnist.train.next_batch(batch_size)\n",
    "    batch_X = batch_X.reshape((batch_size, time_step, n_input))\n",
    "    \n",
    "    sess.run(opti, feed_dict={X: batch_X, y: batch_y, keep_prob: 0.8})\n",
    "    \n",
    "    if iter % 200 == 0:\n",
    "        loss_, acc = sess.run([loss, accuracy], feed_dict={X: batch_X, y: batch_y, keep_prob: 0.8})\n",
    "        print('Iter: ', iter)\n",
    "        print('Accuracy: ', acc, ', Loss: ', loss_)\n",
    "        print('----------------------------------')\n",
    "    iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.921875\n"
     ]
    }
   ],
   "source": [
    "test_X = mnist.test.images[:128].reshape((-1, time_step, n_input))\n",
    "test_y = mnist.test.labels[:128]\n",
    "test_acc = sess.run(accuracy, feed_dict={X: test_X, y: test_y, keep_prob: 1.0})\n",
    "print('Test accuracy: ', test_acc)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習\n",
    "2. 把RNN cell換成GRU觀察Accuracy是否會上升?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRU_cell(hidden_layer_size, X, number_of_layers, dropout=True, dropout_rate=0.8):\n",
    "    def get_GRU(hidden_layer_size, dropout, dropout_rate):\n",
    "        layer = tf.contrib.rnn.GRUCell(hidden_layer_size)\n",
    "        \n",
    "        if dropout:\n",
    "            layer = tf.contrib.rnn.DropoutWrapper(layer, output_keep_prob=dropout_rate)\n",
    "        return layer\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([get_GRU(hidden_layer_size, dropout, dropout_rate) for _ in range(number_of_layers)])\n",
    "    init_state = cell.zero_state(tf.shape(X)[0], tf.float32)\n",
    "    \n",
    "    return cell, init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_graph = tf.Graph()\n",
    "sess = tf.Session(graph=main_graph)\n",
    "\n",
    "with main_graph.as_default():\n",
    "    with tf.name_scope('inputs'):\n",
    "        X = tf.placeholder(tf.float32, shape=[None, time_step, n_input], name='X')\n",
    "        y = tf.placeholder(tf.float32, shape=[None, n_classes], name='y')\n",
    "        keep_prob = tf.placeholder(tf.float32, shape=[], name='keep_prob')\n",
    "        \n",
    "    with tf.variable_scope('GRU_layer'):\n",
    "        cell, init_state = GRU_cell(hidden_layer_size, X, number_of_layers, dropout, keep_prob)\n",
    "        outputs, states = tf.nn.dynamic_rnn(cell, X, initial_state=init_state)\n",
    "        \n",
    "    with tf.variable_scope('output_layer'):\n",
    "        GRU_last_layer = outputs[:, -1, :]\n",
    "        prediction = tf.layers.dense(inputs=GRU_last_layer, units=n_classes)\n",
    "        \n",
    "    with tf.variable_scope('loss'):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=y))\n",
    "        \n",
    "    with tf.name_scope('optimizer'):\n",
    "        opti = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "        \n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  200\n",
      "Accuracy:  0.65625 , Loss:  0.979548\n",
      "----------------------------------\n",
      "Iter:  400\n",
      "Accuracy:  0.773438 , Loss:  0.694255\n",
      "----------------------------------\n",
      "Iter:  600\n",
      "Accuracy:  0.929688 , Loss:  0.380879\n",
      "----------------------------------\n",
      "Iter:  800\n",
      "Accuracy:  0.953125 , Loss:  0.206276\n",
      "----------------------------------\n",
      "Iter:  1000\n",
      "Accuracy:  0.9375 , Loss:  0.2627\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "sess.run(init)\n",
    "\n",
    "iter = 1\n",
    "while iter < 1001:\n",
    "    \n",
    "    batch_X, batch_y = mnist.train.next_batch(batch_size)\n",
    "    batch_X = batch_X.reshape((batch_size, time_step, n_input))\n",
    "    \n",
    "    sess.run(opti, feed_dict={X: batch_X, y: batch_y, keep_prob: 0.8})\n",
    "    \n",
    "    if iter % 200 == 0:\n",
    "        loss_, acc = sess.run([loss, accuracy], feed_dict={X: batch_X, y: batch_y, keep_prob: 0.8})\n",
    "        print('Iter: ', iter)\n",
    "        print('Accuracy: ', acc, ', Loss: ', loss_)\n",
    "        print('----------------------------------')\n",
    "    iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.96875\n"
     ]
    }
   ],
   "source": [
    "test_X = mnist.test.images[:128].reshape((-1, time_step, n_input))\n",
    "test_y = mnist.test.labels[:128]\n",
    "test_acc = sess.run(accuracy, feed_dict={X: test_X, y: test_y, keep_prob: 1.0})\n",
    "print('Test accuracy: ', test_acc)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------\n",
    "\n",
    "把手寫數字兩張平行輸入到model預測，輸入資料變成time_step = 28, input_data_dimension = 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f71a33e8748>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "img1 = mnist.train.images[2].reshape(28, 28)\n",
    "img2 = mnist.train.images[3].reshape(28, 28)\n",
    "img = np.concatenate((img1, img2), axis=1)\n",
    "plt.imshow(img, cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = 28\n",
    "n_input = 56\n",
    "hidden_layer_size = 32\n",
    "number_of_layers = 1\n",
    "dropout = True\n",
    "dropout_rate=0.8\n",
    "learning_rate = 0.001\n",
    "n_classes = 10\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRU_cell(hidden_layer_size, X, number_of_layers, dropout=True, dropout_rate=0.8):\n",
    "    def get_GRU(hidden_layer_size, dropout, dropout_rate):\n",
    "        layer = tf.contrib.rnn.GRUCell(hidden_layer_size)\n",
    "        \n",
    "        if dropout:\n",
    "            layer = tf.contrib.rnn.DropoutWrapper(layer, output_keep_prob=dropout_rate)\n",
    "        return layer\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([get_GRU(hidden_layer_size, dropout, dropout_rate) for _ in range(number_of_layers)])\n",
    "    init_state = cell.zero_state(tf.shape(X)[0], tf.float32)\n",
    "    \n",
    "    return cell, init_state\n",
    "\n",
    "def output_layer(outputs, n_classes):\n",
    "    last_layer = outputs[:, -1, :]\n",
    "    prediction = tf.layers.dense(inputs=last_layer, units=n_classes)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_graph = tf.Graph()\n",
    "sess = tf.Session(graph=main_graph)\n",
    "\n",
    "with main_graph.as_default():\n",
    "    with tf.name_scope('inputs'):\n",
    "        X = tf.placeholder(tf.float32, shape=[None, time_step, n_input], name='X')\n",
    "        y1 = tf.placeholder(tf.float32, shape=[None, n_classes], name='y1')\n",
    "        y2 = tf.placeholder(tf.float32, shape=[None, n_classes], name='y2')\n",
    "        keep_prob = tf.placeholder(tf.float32, shape=[], name='keep_prob')\n",
    "        \n",
    "    with tf.variable_scope('GRU_layer'):\n",
    "        cell, init_state = GRU_cell(hidden_layer_size, X, number_of_layers, dropout, keep_prob)\n",
    "        outputs, states = tf.nn.dynamic_rnn(cell, X, initial_state=init_state)\n",
    "        \n",
    "    with tf.variable_scope('output1_layer'):\n",
    "        prediction1 = output_layer(outputs, n_classes)    \n",
    "    with tf.variable_scope('output2_layer'):\n",
    "        prediction2 = output_layer(outputs, n_classes)\n",
    "        \n",
    "    with tf.variable_scope('loss'):\n",
    "        loss1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction1, labels=y1))\n",
    "        loss2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction2, labels=y2))\n",
    "        loss = loss1 + loss2\n",
    "        \n",
    "    with tf.name_scope('optimizer'):\n",
    "        opti = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "        \n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.logical_and(tf.equal(tf.argmax(prediction1, 1), tf.argmax(y1, 1)), tf.equal(tf.argmax(prediction2, 1), tf.argmax(y2, 1)))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  2000\n",
      "Accuracy:  0.710938 , Loss:  0.914318\n",
      "----------------------------------\n",
      "Iter:  4000\n",
      "Accuracy:  0.882812 , Loss:  0.576342\n",
      "----------------------------------\n",
      "Iter:  6000\n",
      "Accuracy:  0.859375 , Loss:  0.566588\n",
      "----------------------------------\n",
      "Iter:  8000\n",
      "Accuracy:  0.882812 , Loss:  0.444667\n",
      "----------------------------------\n",
      "Iter:  10000\n",
      "Accuracy:  0.921875 , Loss:  0.258174\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "sess.run(init)\n",
    "\n",
    "iter = 1\n",
    "while iter < 10001:\n",
    "    \n",
    "    batch_X1, batch_y1 = mnist.train.next_batch(batch_size)\n",
    "    batch_X2, batch_y2 = mnist.train.next_batch(batch_size)\n",
    "    batch_X1 = batch_X1.reshape(128, 28, 28)\n",
    "    batch_X2 = batch_X2.reshape(128, 28, 28)\n",
    "    batch_X = np.concatenate((batch_X1, batch_X2), axis=2)\n",
    "    \n",
    "    sess.run(opti, feed_dict={X: batch_X, y1: batch_y1, y2: batch_y2, keep_prob: 0.8})\n",
    "    \n",
    "    if iter % 2000 == 0:\n",
    "        loss_, acc = sess.run([loss, accuracy], feed_dict={X: batch_X, y1: batch_y1, y2: batch_y2, keep_prob: 0.8})\n",
    "        print('Iter: ', iter)\n",
    "        print('Accuracy: ', acc, ', Loss: ', loss_)\n",
    "        print('----------------------------------')\n",
    "    iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
